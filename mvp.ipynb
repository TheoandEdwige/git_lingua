{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9df2adae-8ee0-4b64-8169-fae837b41061",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import warnings\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from collections import Counter\n",
    "from env import github_token, github_username\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from acquire import search_github_repositories, get_repo\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "\n",
    "\n",
    "import modeling as m\n",
    "import wrangle as w\n",
    "import prepare as p\n",
    "import explore as e\n",
    "\n",
    "sns.set(style=\"whitegrid\")\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49f09d27-a067-4819-b04f-aa29007d5eb5",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "## Project Overview\n",
    "In this project, we aim to predict the main programming language of a GitHub repository based on the text of its README file. This project has several goals:\n",
    "\n",
    "1. Collect data from GitHub repositories.\n",
    "2. Perform exploratory data analysis on the READMEs to understand their characteristics.\n",
    "3. Process and structure the READMEs for analysis.\n",
    "4. Build and evaluate machine learning models for programming language prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0445d4cb-6ad2-4e2e-83c5-a0b33163dee9",
   "metadata": {},
   "source": [
    "## Project Goals and Deliverables\n",
    "The goals of this project include:\n",
    "- Building a machine learning model for text classification.\n",
    "- Gaining insights into the relationships between README text and programming languages.\n",
    "- Creating a well-documented Jupyter Notebook.\n",
    "- Preparing presentation slides summarizing the project's findings."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "210a792c-e417-4e61-ac40-45cb02d10252",
   "metadata": {},
   "source": [
    "#### Data Source\n",
    "We will collect data from artificial intelligence related GitHub repositories using web scraping, and API methods."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87184e0d-f21d-412b-a65e-3c14e4cb6f7e",
   "metadata": {},
   "source": [
    "## Acquire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "326ea415-1405-4b76-9e44-225a66572738",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the 'get_repo' function to fetch repository data related to the topic \"artificial intelligence\"\n",
    "df = get_repo(\"artificial intelligence\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6d7f23a0-1624-4930-aaa4-095a8eabffa3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>Language</th>\n",
       "      <th>Readme</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>awesome-artificial-intelligence</td>\n",
       "      <td>NaN</td>\n",
       "      <td># Awesome Artificial Intelligence (AI) [![Awes...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Artificial-Intelligence</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All about AI with Cheat-Sheets(+100 Cheat-shee...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>opencog</td>\n",
       "      <td>Scheme</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Artificial-Intelligence-Deep-Learning-Machine-...</td>\n",
       "      <td>Python</td>\n",
       "      <td># NEW LIST 2023 - 2024: Machine-Learning / Dee...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>artificial-intelligence</td>\n",
       "      <td>Python</td>\n",
       "      <td># Artificial Intelligence Nanodegree Program R...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Name Language  \\\n",
       "0                    awesome-artificial-intelligence      NaN   \n",
       "1                            Artificial-Intelligence      NaN   \n",
       "2                                            opencog   Scheme   \n",
       "3  Artificial-Intelligence-Deep-Learning-Machine-...   Python   \n",
       "4                            artificial-intelligence   Python   \n",
       "\n",
       "                                              Readme  \n",
       "0  # Awesome Artificial Intelligence (AI) [![Awes...  \n",
       "1  All about AI with Cheat-Sheets(+100 Cheat-shee...  \n",
       "2                                                NaN  \n",
       "3  # NEW LIST 2023 - 2024: Machine-Learning / Dee...  \n",
       "4  # Artificial Intelligence Nanodegree Program R...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08f519a9-320d-475b-8f55-104317d9130f",
   "metadata": {},
   "source": [
    "## Prepare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e60857e9-c846-4287-b051-a66d1d6cc6d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert all column names to lowercase\n",
    "# Remove rows with missing 'readme' values\n",
    "# Check for and remove duplicate rows\n",
    "# Reset the index after dropping rows\n",
    "df = w.convert_and_dropna(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "504adeef-3ad8-4612-a336-787514160d7d",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 7\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Preprocess the text data in the 'readme' column of the DataFrame\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# The preprocess_text_in_dataframe function performs text cleaning and transformation,\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# which may include tasks like lowering text, removing special characters, tokenization, stemming,\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# or lemmatization, and removing stopwords, depending on the implementation of the function.\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# The processed text is then assigned back to the 'readme' column in the DataFrame.\u001b[39;00m\n\u001b[1;32m      6\u001b[0m stop_words \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mai\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124martificial intelligence\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmachine learning\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdeep learning\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m----> 7\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpreprocess_text_in_dataframe\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mreadme\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_words\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop_words\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/git_lingua/prepare.py:45\u001b[0m, in \u001b[0;36mpreprocess_text_in_dataframe\u001b[0;34m(dataframe, column_name, extra_words, exclude_words)\u001b[0m\n\u001b[1;32m     43\u001b[0m extra_words \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mai\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124martificial\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mintelligence\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmachinelearning\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdeep learning\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     44\u001b[0m \u001b[38;5;66;03m# Basic cleaning\u001b[39;00m\n\u001b[0;32m---> 45\u001b[0m dataframe[column_name] \u001b[38;5;241m=\u001b[39m \u001b[43mdataframe\u001b[49m\u001b[43m[\u001b[49m\u001b[43mcolumn_name\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mapply(basic_clean)\n\u001b[1;32m     47\u001b[0m \u001b[38;5;66;03m# Tokenization\u001b[39;00m\n\u001b[1;32m     48\u001b[0m dataframe[column_name] \u001b[38;5;241m=\u001b[39m dataframe[column_name]\u001b[38;5;241m.\u001b[39mapply(tokenize)\n",
      "\u001b[0;31mTypeError\u001b[0m: 'NoneType' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "# Preprocess the text data in the 'readme' column of the DataFrame\n",
    "# The preprocess_text_in_dataframe function performs text cleaning and transformation,\n",
    "# which may include tasks like lowering text, removing special characters, tokenization, stemming,\n",
    "# or lemmatization, and removing stopwords, depending on the implementation of the function.\n",
    "# The processed text is then assigned back to the 'readme' column in the DataFrame.\n",
    "stop_words = ['ai', 'artificial intelligence', 'machine learning', 'deep learning']\n",
    "df = p.preprocess_text_in_dataframe(df, 'readme', extra_words=stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "666b1fd2-ef82-4b6c-b475-f89810d79d60",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a965a3a-167c-4ef8-8d56-0a3337584c10",
   "metadata": {},
   "source": [
    "## Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab692254-afb0-4f8e-9682-bcadad9fcbf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate basic statistics (word count, character count, average word length) for the 'readme' column in the DataFrame 'df'\n",
    "basic_stats = e.calculate_basic_statistics(df, 'readme')\n",
    "\n",
    "# Display the basic statistics\n",
    "basic_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "865c41d1-c620-47b6-90de-7b1b510b472b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#top words in the readme column\n",
    "top_words = e.identify_most_common_words(df, 'readme', top_n=10)\n",
    "top_words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5ae7f7f-0c03-4442-bebd-9cac7585ce62",
   "metadata": {},
   "source": [
    "#### Most Common Words in READMEs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeceb8dc-59e7-45f6-ae14-c1287db2ce2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#visualization of the top words in readme\n",
    "e.top_words_barplot(top_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88aad1be-2834-4b55-b8d9-8458aa8dcc96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the function to plot the average unique words by programming language\n",
    "# Provide the DataFrame 'df' containing 'language' and 'readme' columns\n",
    "w.plot_unique_words_by_language(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd2582e5-efa6-4143-ace2-38a2a02aa027",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the function to display the top 3 unique words for the most popular programming languages\n",
    "# Provide the DataFrame 'df' containing 'language' and 'readme' columns\n",
    "w.top_unique_words_by_language(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e171b5a0-bfed-4b8a-8edc-693f5112ebdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#wordcloud of the words in the readme column\n",
    "e.generate_word_cloud(df, 'readme')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf2adcfd-3dd6-47b3-a5d4-4b2b83a7fac0",
   "metadata": {},
   "source": [
    "## **Data Exploration Summary**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbd2f27d-399b-435c-b603-df8999204d45",
   "metadata": {},
   "source": [
    "In the initial phase of our project, we conducted a comprehensive data exploration of the dataset to gain insights and understand the characteristics of the data. This process involved calculating basic statistics for the 'readme' column, identifying the most common words in the 'readme' text, and determining the top unique words associated with popular programming languages.\n",
    "\n",
    "1. **Basic Statistics for 'readme' Column:**\n",
    "   - We found that the 'readme' column contains a total of 784 entries.\n",
    "   - However, there are only 783 unique entries, indicating that there is one instance with an empty 'readme' text.\n",
    "   - The most frequent entry in this column is an empty string, which occurs twice.\n",
    "\n",
    "2. **Top Words in 'readme' Column:**\n",
    "   - To understand the most prevalent words in the 'readme' text, we identified the top 10 words.\n",
    "   - The top five words are \"Learning\" (3913 occurrences), \"Data\" (2693 occurrences), \"Machine\" (2213 occurrences), \"(Whitespace)\" (2051 occurrences), and \"Artificial\" (1926 occurrences).\n",
    "\n",
    "3. **Top Unique Words by Programming Language:**\n",
    "   - We conducted an analysis to find the top 3 unique words associated with the most popular programming languages in the dataset.\n",
    "   - For example, in the case of Python, the top 3 unique words are \"python\" (TF-IDF score:  0.071), \"artificial\" (TF-IDF score: 0.059), and \"intelligence\" (TF-IDF score: 0.058).\n",
    "   - Similar analyses were performed for Jupyter Notebook, JavaScript, Java, and C++.\n",
    "\n",
    "These exploratory findings provide an essential foundation for our project, allowing us to understand the dataset's composition and gain insights into the significant terms and patterns within the 'readme' text. This information will guide our subsequent steps in natural language processing and machine learning model development."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f01a6b7-d04c-4cbd-8913-a067c5e6463f",
   "metadata": {},
   "source": [
    "### Encoding the target variable and split the data into training and testing sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10beb982-81a7-4b07-8b57-6f9981800855",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The function returns X_train, X_val, y_train, and y_val, representing the training and validation sets.\n",
    "X_train, X_val, y_train, y_val = m.encode_and_split_data(df, text_column='readme', target_column='language', test_size=0.2, random_state=42)\n",
    "\n",
    "# Print the shapes of the training and validation sets\n",
    "print(\"Training set shape:\", X_train.shape, y_train.shape)\n",
    "print(\"Validation set shape:\", X_val.shape, y_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9211ae8-85da-4a83-9161-16f99fa65ada",
   "metadata": {},
   "source": [
    "### Feature Engineering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f97cbe8-a773-4b11-af11-4037f6a5f8c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The function returns X_train_tfidf and X_val_tfidf, representing the training and validation sets transformed into TF-IDF vectors.\n",
    "X_train_tfidf, X_val_tfidf = m.tfidf_vectorization(X_train, X_val)\n",
    "\n",
    "# Print the shapes of the TF-IDF transformed sets\n",
    "print(\"Training set (TF-IDF) shape:\", X_train_tfidf.shape)\n",
    "print(\"Validation set (TF-IDF) shape:\", X_val_tfidf.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c5a80f4-cb35-4711-90a1-19bf4adbf726",
   "metadata": {},
   "source": [
    "## Model Selection and Training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a39831b8-539a-4f31-add6-9c9cc31f2b81",
   "metadata": {},
   "source": [
    "#### Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdcc2473-8a17-40be-bb30-a9fe054ea980",
   "metadata": {},
   "outputs": [],
   "source": [
    "m.baseline(X_train, y_train, X_val, y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc771b58-b373-45d1-b9ab-9da3ddfe89e8",
   "metadata": {},
   "source": [
    "### Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "610adb52-1108-46c7-9c1c-8b77332d723b",
   "metadata": {},
   "outputs": [],
   "source": [
    "m.train_decision_tree(X_train_tfidf, y_train, X_val_tfidf, y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f00339a-51f4-4d97-bc5e-5cc49025ebcf",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c36434af-97eb-4258-a917-a634db767581",
   "metadata": {},
   "outputs": [],
   "source": [
    "m.train_random_forest(X_train_tfidf, y_train, X_val_tfidf, y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "214a144e-cb39-4524-8252-09388cd49de7",
   "metadata": {},
   "source": [
    "### K-Nearest Neighbors (KNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f477884d-64dc-4e26-b649-65ba75b4355e",
   "metadata": {},
   "outputs": [],
   "source": [
    "m.train_knn(X_train_tfidf, y_train, X_val_tfidf, y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f40adc13-dead-4f9d-a0db-d4edb4c72cc1",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6b2a8f1-893d-429d-ba4f-951fd1f54492",
   "metadata": {},
   "outputs": [],
   "source": [
    "m.train_logistic_regression(X_train_tfidf, y_train, X_val_tfidf, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f6b3ae1-b686-4dad-928e-125ad6758270",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_comparison_result = m.evaluate_and_compare_models(X_train_tfidf, y_train, X_val_tfidf, y_val)\n",
    "print(model_comparison_result)\n",
    "\n",
    "# Create visualizations (e.g., bar charts)\n",
    "model_comparison_result.set_index('Model').plot(kind='bar', subplots=True, layout=(2, 2), legend=False, figsize=(12, 8))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8660e3c0-66cd-4dad-9d39-1eb1438f929a",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": null,
   "id": "a5c2b7e7-b4b7-4bfd-8fb2-991ce34b2554",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set_accuracy = m.evaluate_final_model(X_train, y_train, X_val, y_val, random_state=42)\n",
    "print(f\"Test Set Accuracy: {test_set_accuracy:.4f}\")"
=======
   "execution_count": 1,
   "id": "a5c2b7e7-b4b7-4bfd-8fb2-991ce34b2554",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ntest_set_accuracy = evaluate_final_model(X_train, y_train, X_val, y_val, random_state=42)\\nprint(f\"Test Set Accuracy: {test_set_accuracy:.4f}\")\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "test_set_accuracy = evaluate_final_model(X_train, y_train, X_val, y_val, random_state=42)\n",
    "print(f\"Test Set Accuracy: {test_set_accuracy:.4f}\")\n",
    "\"\"\""
>>>>>>> 0b639d397bf522f5618a700c6ca8ab5f54dc2999
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ec4eb13-f238-405a-9638-6999e202b883",
   "metadata": {},
   "source": [
    "## **Project Model Summary**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90ba8ca8-4146-4a75-9031-25b6f0f6ad45",
   "metadata": {},
   "source": [
    "We have developed and evaluated multiple machine learning models for predicting the main programming language of a repository based on the README text. This summary provides insights into the model development and their performance on the validation dataset.\n",
    "\n",
    "1. **Data Preparation:**\n",
    "   - The data has been split into training and validation sets using the `encode_and_split_data` function.\n",
    "   - Training set shape: (627,) (627,)\n",
    "   - Validation set shape: (157,) (157,)\n",
    "\n",
    "2. **TF-IDF Vectorization:**\n",
    "   - The text data has been transformed into TF-IDF vectors for feature representation using the `tfidf_vectorization` function.\n",
    "   - Training set (TF-IDF) shape: (627, 5000)\n",
    "   - Validation set (TF-IDF) shape: (157, 5000)\n",
    "\n",
    "3. **Model Training and Validation:**\n",
    "   - We trained and evaluated four different models on the validation dataset.\n",
    "\n",
    "4. **Decision Tree Model:**\n",
    "   - Accuracy: 0.4013\n",
    "   - Precision: 0.3563\n",
    "   - Recall: 0.4013\n",
    "   - F1-Score: 0.3748\n",
    "\n",
    "5. **Random Forest Model:**\n",
    "   - Accuracy: 0.4331\n",
    "   - Precision: 0.2925\n",
    "   - Recall: 0.4331\n",
    "   - F1-Score: 0.3226\n",
    "\n",
    "6. **K-Nearest Neighbors (KNN) Model:**\n",
    "   - Accuracy: 0.2611\n",
    "   - Precision: 0.3797\n",
    "   - Recall: 0.2611\n",
    "   - F1-Score: 0.1435\n",
    "\n",
    "7. **Logistic Regression Model:**\n",
    "   - Accuracy: 0.3949\n",
    "   - Precision: 0.2889\n",
    "   - Recall: 0.3949\n",
    "   - F1-Score: 0.3095\n",
    "\n",
    "8. **Model Comparison:**\n",
    "   - The table below summarizes the performance of each model on the validation dataset.\n",
    "\n",
    "|       Model        | Accuracy | Precision | Recall | F1-Score |\n",
    "|-------------------|----------|-----------|--------|----------|\n",
    "| Decision Tree     | 0.4013   | 0.3563    | 0.4013 | 0.3748   |\n",
    "| Random Forest     | 0.4331   | 0.2925    | 0.4331 | 0.3226   |\n",
    "| KNN               | 0.2611   | 0.3797    | 0.2611 | 0.1435   |\n",
    "| Logistic Regression | 0.3949   | 0.2889    | 0.3949 | 0.3095   |\n",
    "\n",
    "9. **Test Set Accuracy:**\n",
    "   - The best-performing model, the Random Forest Classifier, achieved an accuracy of 0.3291 on the test dataset.\n",
    "\n",
    "These results provide valuable insights into the model performance and will guide our final model selection and deployment in the project."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "553215f0-24ab-4f60-adae-58fb2246bb2d",
   "metadata": {},
   "source": [
    "# Overall Project Conclusion\n",
    "\n",
    "## Project Goals and Approach\n",
    "\n",
    "The goal of this project was to develop a predictive model that identifies the main programming language of a repository based on the README text. To achieve this goal, we followed a structured approach:\n",
    "\n",
    "1. **Data Collection**: We obtained data from GitHub repositories using the GitHub API, collecting information such as the repository name, description, and README text. Our goal was to gather a diverse dataset that represents various programming languages.\n",
    "\n",
    "2. **Data Exploration**: We conducted an in-depth exploration of the data to understand its characteristics. We calculated basic statistics such as word count, character count, and average word length in the README texts. Additionally, we identified the most common words in the dataset and examined the unique words used for each programming language.\n",
    "\n",
    "## Key Findings\n",
    "\n",
    "### Data Exploration\n",
    "\n",
    "Our data exploration revealed several key findings:\n",
    "\n",
    "- The dataset contained a total of 784 README texts with 783 unique texts. However, two texts were identical.\n",
    "- The most common words in the README texts included \"learning,\" \"data,\" \"machine,\" and others, highlighting their prevalence in the programming community.\n",
    "- The analysis of unique words showed distinct patterns for different programming languages. For example, \"Python\" was highly associated with Python-related READMEs.\n",
    "\n",
    "### Model Development\n",
    "\n",
    "We trained and evaluated four machine learning models on the data:\n",
    "\n",
    "- Decision Tree\n",
    "- Random Forest\n",
    "- K-Nearest Neighbors (KNN)\n",
    "- Logistic Regression\n",
    "\n",
    "The models were assessed based on accuracy, precision, recall, and F1-score on a validation dataset. The Random Forest model outperformed the others, achieving an accuracy of 0.4331.\n",
    "\n",
    "## Recommendations\n",
    "\n",
    "Based on our findings, we make the following recommendations:\n",
    "\n",
    "1. **Model Selection**: The Random Forest model has demonstrated the highest accuracy. We recommend selecting this model for predicting programming languages based on README text.\n",
    "\n",
    "2. **Enhanced Data Collection**: To further improve model performance, we recommend expanding the dataset by collecting README texts from a more extensive and diverse set of repositories.\n",
    "\n",
    "3. **Hyperparameter Tuning**: For the selected model, fine-tuning the hyperparameters and conducting cross-validation can lead to even better performance.\n",
    "\n",
    "4. **Deployment**: Once the final model is selected, consider deploying it as a prediction tool for developers. It can assist users in automatically tagging their repositories with the correct programming language.\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "If we had more time and resources, we would consider the following next steps:\n",
    "\n",
    "1. **Enhanced Data Preprocessing**: Implement more advanced text preprocessing techniques, such as handling punctuation, stemming, or lemmatization to improve text data quality.\n",
    "\n",
    "2. **Model Interpretability**: Analyze feature importance in the Random Forest model to gain insights into which terms play a significant role in predicting programming languages.\n",
    "\n",
    "3. **Continuous Data Collection**: Develop an automated data collection pipeline that continuously updates the dataset with recent GitHub repositories and READMEs.\n",
    "\n",
    "4. **User Interface**: Create a user-friendly interface for developers to interact with the model and automatically label their repositories.\n",
    "\n",
    "This project has laid the foundation for a valuable tool that can assist developers and the programming community. By implementing the recommendations and next steps, we can refine and expand this tool to further contribute to the developer community."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
